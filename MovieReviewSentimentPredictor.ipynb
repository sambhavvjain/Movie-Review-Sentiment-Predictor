{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6084xqQH65_2",
        "outputId": "4eb35f2f-84b4-4720-cb58-d5e7df2fbb62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ishaa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from numpy import array\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I3_EGKDi69JW"
      },
      "outputs": [],
      "source": [
        "def load_doc(filename):\n",
        " # open the file as read only\n",
        " file = open(filename, 'r')\n",
        " # read all text\n",
        " text = file.read()\n",
        " # close the file\n",
        " file.close()\n",
        " return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ksclla5z7D8G"
      },
      "outputs": [],
      "source": [
        "def clean_doc(doc):\n",
        " # split into tokens by white space\n",
        " tokens = doc.split()\n",
        " # remove punctuation from each token\n",
        " table = str.maketrans('', '', punctuation)\n",
        " tokens = [w.translate(table) for w in tokens]\n",
        " # remove remaining tokens that are not alphabetic\n",
        " tokens = [word for word in tokens if word.isalpha()]\n",
        " # filter out stop words\n",
        " stop_words = set(stopwords.words('english'))\n",
        " tokens = [w for w in tokens if not w in stop_words]\n",
        " # filter out short tokens\n",
        " tokens = [word for word in tokens if len(word) > 1]\n",
        " return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KRmS4vP2-bmN"
      },
      "outputs": [],
      "source": [
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        " # load doc\n",
        " doc = load_doc(filename)\n",
        " # clean doc\n",
        " tokens = clean_doc(doc)\n",
        " # update counts\n",
        " vocab.update(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fTva0VQi-gu6"
      },
      "outputs": [],
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        " # walk through all files in the folder\n",
        " for filename in listdir(directory):\n",
        "  # skip any reviews in the test set\n",
        "  if filename.startswith('cv9'):\n",
        "    continue\n",
        "  # create the full path of the file to open\n",
        "  path = directory + '/' + filename\n",
        "  # add doc to vocab\n",
        "  add_doc_to_vocab(path, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0touJl6-kRv",
        "outputId": "49dfff81-8b55-4cff-fb21-d2bf1a28d0e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ]
        }
      ],
      "source": [
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('Dataset/review_polarity/txt_sentoken/pos', vocab)\n",
        "process_docs('Dataset/review_polarity/txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGJFzs9p72pl"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05T1Dv9y_BlW",
        "outputId": "a5a23000-4242-4bb6-da8b-13bcabce52c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25767\n"
          ]
        }
      ],
      "source": [
        "# keep tokens with a min occurrence\n",
        "min_occurance = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5ICZorrx-1jP"
      },
      "outputs": [],
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        " # convert lines to a single blob of text\n",
        " data = '\\n'.join(lines)\n",
        " # open file\n",
        " file = open(filename, 'w')\n",
        " # write text\n",
        " file.write(data)\n",
        " # close file\n",
        " file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lgW7FCUT-35n"
      },
      "outputs": [],
      "source": [
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Pg11luIS7GVJ"
      },
      "outputs": [],
      "source": [
        "def doc_to_line(filename, vocab):\n",
        " # load the doc\n",
        " doc = load_doc(filename)\n",
        " # clean doc\n",
        " tokens = clean_doc(doc)\n",
        " # filter by vocab\n",
        " tokens = [w for w in tokens if w in vocab]\n",
        " return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xQQ3783l7KAK"
      },
      "outputs": [],
      "source": [
        "def process_docs(directory, vocab, is_train):\n",
        " lines = list()\n",
        " # walk through all files in the folder\n",
        " for filename in listdir(directory):\n",
        "  # skip any reviews in the test set\n",
        "  if is_train and filename.startswith('cv9'):\n",
        "    continue\n",
        "  if not is_train and not filename.startswith('cv9'):\n",
        "    continue\n",
        "  # create the full path of the file to open\n",
        "  path = directory + '/' + filename\n",
        "  # load and clean the doc\n",
        "  line = doc_to_line(path, vocab)\n",
        "  # add to list\n",
        "  lines.append(line)\n",
        " return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7YX9IDL87oVj"
      },
      "outputs": [],
      "source": [
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        " scores = list()\n",
        " n_repeats = 30\n",
        " n_words = Xtest.shape[1]\n",
        " for i in range(n_repeats):\n",
        "  # define network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
        "  # evaluate\n",
        "  loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "  scores.append(acc)\n",
        "  print('%d accuracy: %s' % ((i+1), acc))\n",
        " return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lJsxqYAZ7wPp"
      },
      "outputs": [],
      "source": [
        "def prepare_data(train_docs, test_docs, mode):\n",
        " # create the tokenizer\n",
        " tokenizer = Tokenizer()\n",
        " # fit the tokenizer on the documents\n",
        " tokenizer.fit_on_texts(train_docs)\n",
        " # encode training data set\n",
        " Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        " # encode training data set\n",
        " Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        " return Xtrain, Xtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEUOeVVC79Y9",
        "outputId": "5fd55b41-e33c-4f90-fb70-ce7cf5dcb3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1800, 25768)\n",
            "(200, 25768)\n"
          ]
        }
      ],
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "positive_lines = process_docs('Dataset/review_polarity/txt_sentoken/pos', vocab, True)\n",
        "negative_lines = process_docs('Dataset/review_polarity/txt_sentoken/neg', vocab, True)\n",
        "train_docs = negative_lines + positive_lines\n",
        "# load all test reviews\n",
        "positive_lines = process_docs('Dataset/review_polarity/txt_sentoken/pos', vocab, False)\n",
        "negative_lines = process_docs('Dataset/review_polarity/txt_sentoken/neg', vocab, False)\n",
        "test_docs = negative_lines + positive_lines\n",
        "# prepare labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "docs = train_docs\n",
        "tokenizer.fit_on_texts(docs)\n",
        "\n",
        "# encode training data set\n",
        "Xtrain = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "print(Xtrain.shape)\n",
        "\n",
        "docs = test_docs\n",
        "# encode training data set\n",
        "Xtest = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "print(Xtest.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qnIkVxcPJ-q-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "w:\\Languages\\Python\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "n_words = Xtest.shape[1]\n",
        "\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfCMl7LNNfE",
        "outputId": "c4ad2ac5-5b0c-49a2-bde8-9d299e7d74eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "57/57 - 2s - 43ms/step - accuracy: 0.5806 - loss: 0.6912\n",
            "Epoch 2/50\n",
            "57/57 - 1s - 22ms/step - accuracy: 0.7556 - loss: 0.6798\n",
            "Epoch 3/50\n",
            "57/57 - 1s - 21ms/step - accuracy: 0.8511 - loss: 0.6590\n",
            "Epoch 4/50\n",
            "57/57 - 1s - 22ms/step - accuracy: 0.9239 - loss: 0.6273\n",
            "Epoch 5/50\n",
            "57/57 - 1s - 20ms/step - accuracy: 0.9211 - loss: 0.5863\n",
            "Epoch 6/50\n",
            "57/57 - 1s - 21ms/step - accuracy: 0.9256 - loss: 0.5416\n",
            "Epoch 7/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9511 - loss: 0.4933\n",
            "Epoch 8/50\n",
            "57/57 - 1s - 21ms/step - accuracy: 0.9544 - loss: 0.4455\n",
            "Epoch 9/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 0.9589 - loss: 0.4008\n",
            "Epoch 10/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9656 - loss: 0.3587\n",
            "Epoch 11/50\n",
            "57/57 - 1s - 17ms/step - accuracy: 0.9717 - loss: 0.3210\n",
            "Epoch 12/50\n",
            "57/57 - 1s - 17ms/step - accuracy: 0.9811 - loss: 0.2871\n",
            "Epoch 13/50\n",
            "57/57 - 1s - 16ms/step - accuracy: 0.9844 - loss: 0.2573\n",
            "Epoch 14/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9856 - loss: 0.2306\n",
            "Epoch 15/50\n",
            "57/57 - 1s - 20ms/step - accuracy: 0.9894 - loss: 0.2064\n",
            "Epoch 16/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9900 - loss: 0.1855\n",
            "Epoch 17/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 0.9922 - loss: 0.1673\n",
            "Epoch 18/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9939 - loss: 0.1508\n",
            "Epoch 19/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9939 - loss: 0.1358\n",
            "Epoch 20/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9961 - loss: 0.1227\n",
            "Epoch 21/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9967 - loss: 0.1114\n",
            "Epoch 22/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9978 - loss: 0.1010\n",
            "Epoch 23/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9983 - loss: 0.0916\n",
            "Epoch 24/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 0.9989 - loss: 0.0837\n",
            "Epoch 25/50\n",
            "57/57 - 1s - 16ms/step - accuracy: 0.9989 - loss: 0.0765\n",
            "Epoch 26/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0698\n",
            "Epoch 27/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0639\n",
            "Epoch 28/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0590\n",
            "Epoch 29/50\n",
            "57/57 - 1s - 17ms/step - accuracy: 1.0000 - loss: 0.0541\n",
            "Epoch 30/50\n",
            "57/57 - 1s - 17ms/step - accuracy: 1.0000 - loss: 0.0498\n",
            "Epoch 31/50\n",
            "57/57 - 1s - 17ms/step - accuracy: 1.0000 - loss: 0.0458\n",
            "Epoch 32/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0423\n",
            "Epoch 33/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 1.0000 - loss: 0.0391\n",
            "Epoch 34/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 1.0000 - loss: 0.0361\n",
            "Epoch 35/50\n",
            "57/57 - 1s - 21ms/step - accuracy: 1.0000 - loss: 0.0336\n",
            "Epoch 36/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 1.0000 - loss: 0.0313\n",
            "Epoch 37/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 1.0000 - loss: 0.0290\n",
            "Epoch 38/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0270\n",
            "Epoch 39/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0252\n",
            "Epoch 40/50\n",
            "57/57 - 1s - 17ms/step - accuracy: 1.0000 - loss: 0.0235\n",
            "Epoch 41/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0221\n",
            "Epoch 42/50\n",
            "57/57 - 1s - 18ms/step - accuracy: 1.0000 - loss: 0.0206\n",
            "Epoch 43/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 1.0000 - loss: 0.0193\n",
            "Epoch 44/50\n",
            "57/57 - 1s - 19ms/step - accuracy: 1.0000 - loss: 0.0182\n",
            "Epoch 45/50\n",
            "57/57 - 1s - 22ms/step - accuracy: 1.0000 - loss: 0.0170\n",
            "Epoch 46/50\n",
            "57/57 - 1s - 21ms/step - accuracy: 1.0000 - loss: 0.0160\n",
            "Epoch 47/50\n",
            "57/57 - 1s - 17ms/step - accuracy: 1.0000 - loss: 0.0151\n",
            "Epoch 48/50\n",
            "57/57 - 1s - 16ms/step - accuracy: 1.0000 - loss: 0.0142\n",
            "Epoch 49/50\n",
            "57/57 - 1s - 16ms/step - accuracy: 1.0000 - loss: 0.0134\n",
            "Epoch 50/50\n",
            "57/57 - 1s - 16ms/step - accuracy: 1.0000 - loss: 0.0126\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x275b9d76c90>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=50, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnApt7tRNprT",
        "outputId": "95e8d087-7aea-4503-fba5-14cbe58a5826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 91.000003\n"
          ]
        }
      ],
      "source": [
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bpsXkLv38FEK"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        " # clean\n",
        " tokens = clean_doc(review)\n",
        " # filter by vocab\n",
        " tokens = [w for w in tokens if w in vocab]\n",
        " # convert to line\n",
        " line = ' '.join(tokens)\n",
        " # encode\n",
        " encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
        " # prediction\n",
        " yhat = model.predict(encoded, verbose=0)\n",
        " return round(yhat[0,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vds0s-rIOess",
        "outputId": "cd92c838-6438-4b50-e557-c959db6b6874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# test positive text\n",
        "text = 'Best movie ever!'\n",
        "print(predict_sentiment(text, vocab, tokenizer, model))\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "print(predict_sentiment(text, vocab, tokenizer, model))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
